{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di DGCNN-AE_V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StefanoBergia/mlnotebooks/blob/main/Copia_di_DGCNN_AE_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7txbjM-ng21"
      },
      "source": [
        "# AIML20 Project - DGCNN AE\n",
        "\n",
        "> Blocco con rientro\n",
        "\n",
        "\n",
        "### Assistant: Antonio Alliegro, antonio.alliegro at polito dot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZQE7t2ercoh",
        "outputId": "9ec4257b-c148-4eee-a25b-0224b97230ca"
      },
      "source": [
        "#pip install trimesh\r\n",
        "#!pip install open3d\r\n",
        "!pip install plotly.express"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting plotly.express\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/d6/8a2906f51e073a4be80cab35cfa10e7a34853e60f3ed5304ac470852a08d/plotly_express-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.6/dist-packages (from plotly.express) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from plotly.express) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from plotly.express) (1.19.5)\n",
            "Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from plotly.express) (4.4.1)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.6/dist-packages (from plotly.express) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from plotly.express) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.5->plotly.express) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.1.0->plotly.express) (1.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->plotly.express) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.0->plotly.express) (2018.9)\n",
            "Installing collected packages: plotly.express\n",
            "Successfully installed plotly.express\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdMzmw6mJ16k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "882a6207-c76a-4b5c-ddfd-57985722b20d"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from google.colab import drive\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import json\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \"\"\"\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-77302c217f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWnlTtquZSa2"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j16xhIKxZXlf"
      },
      "source": [
        "class ShapeNetDataset(data.Dataset):\r\n",
        "    def __init__(self,\r\n",
        "                 root,\r\n",
        "                 npoints=1024,      #previously 2500\r\n",
        "                 classification=False,\r\n",
        "                 class_choice=None,\r\n",
        "                 split='train',\r\n",
        "                 data_augmentation=False):\r\n",
        "        self.npoints = npoints                                              #number of poits to random sample\r\n",
        "        self.root = root                                                    #root folder for the dataset\r\n",
        "        self.catfile = os.path.join(self.root, 'synsetoffset2category.txt') #contains the mapping between class name and folder name\r\n",
        "        self.cat = {}                                                       #dictionary that maps class name and folders\r\n",
        "        self.data_augmentation = data_augmentation                          #we don't use it\r\n",
        "        self.classification = classification                                #flag to decide between classification and segmentation\r\n",
        "        self.seg_classes = {}                                               #maps for each class name the number of segment possible for that class\r\n",
        "        self.meta = {}                                                      #maps each class to the list of path of the pointclouds\r\n",
        "\r\n",
        "\r\n",
        "        with open(self.catfile, 'r') as f:                                  #initialize self.cat to map classname and folder\r\n",
        "            for line in f:\r\n",
        "                ls = line.strip().split()\r\n",
        "                self.cat[ls[0]] = ls[1]\r\n",
        "  \r\n",
        "        if not class_choice is None:                                        #filters only the classes we are interested in\r\n",
        "            self.cat = {k: v for k, v in self.cat.items() if k in class_choice}\r\n",
        "\r\n",
        "        self.id2cat = {v: k for k, v in self.cat.items()}                   #reverse mapping between classes and folder\r\n",
        "\r\n",
        "        splitfile = os.path.join(self.root, 'train_test_split', 'shuffled_{}_file_list.json'.format(split)) #opens the train/validation/test file\r\n",
        "        filelist = json.load(open(splitfile, 'r'))\r\n",
        "        for item in self.cat:           #init class label\r\n",
        "            self.meta[item] = []        \r\n",
        "\r\n",
        "        for file in filelist:\r\n",
        "            _, category, uuid = file.split('/')\r\n",
        "            if category in self.cat.values():\r\n",
        "                self.meta[self.id2cat[category]].append((os.path.join(self.root, category, 'points', uuid+'.pts'),  #appends path for pointcloud\r\n",
        "                                        os.path.join(self.root, category, 'points_label', uuid+'.seg')))            #appends path for segmentation of points\r\n",
        "\r\n",
        "        self.datapath = []\r\n",
        "        for item in self.cat:\r\n",
        "            for fn in self.meta[item]:\r\n",
        "                self.datapath.append((item, fn[0], fn[1]))          #transform meta in an array to be used with an integer index in getitem\r\n",
        "\r\n",
        "        self.classes = dict(zip(sorted(self.cat), range(len(self.cat))))  #create a dictionary with key=category value=id between 0 and number of categories\r\n",
        "        #print(self.classes) #{'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15}\r\n",
        "        with open(os.path.join(self.root,'num_seg_classes.txt'), 'r') as f:\r\n",
        "            for line in f:\r\n",
        "                ls = line.strip().split()\r\n",
        "                self.seg_classes[ls[0]] = int(ls[1])      #set the number of segmentation part for each class\r\n",
        "        self.num_seg_classes = self.seg_classes[list(self.cat.keys())[0]]\r\n",
        "        #print(self.seg_classes, self.num_seg_classes)   #{'Airplane': 4, 'Bag': 2, 'Cap': 2, 'Car': 4, 'Chair': 4, 'Earphone': 3, 'Guitar': 3, 'Knife': 2, 'Lamp': 4, 'Laptop': 2, 'Motorbike': 6, 'Mug': 2, 'Pistol': 3, 'Rocket': 3, 'Skateboard': 3, 'Table': 3} 4\r\n",
        "\r\n",
        "\r\n",
        "    def __getitem__(self, index):\r\n",
        "        fn = self.datapath[index]                         #gets an entry in datapath (classname pointcloud_path segment_path)\r\n",
        "        cls = self.classes[self.datapath[index][0]]       #gets class internal id\r\n",
        "        #print(fn[1])\r\n",
        "        point_set = np.loadtxt(fn[1]).astype(np.float32)  #reads the pointcloud file and stores it  in a matrix nx3 (2 dimensions)\r\n",
        "        seg = np.loadtxt(fn[2]).astype(np.int64)          #reads the segmentation \r\n",
        "        #print(point_set.shape, seg.shape)\r\n",
        "        \r\n",
        "        choice = np.random.choice(len(seg), self.npoints, replace=True)   #random sampling (default 1024 points)\r\n",
        "        #resample\r\n",
        "        point_set = point_set[choice, :]\r\n",
        "\r\n",
        "        point_set = point_set - np.expand_dims(np.mean(point_set, axis = 0), 0) #center, subtract the mean of each coordinate to each point\r\n",
        "        dist = np.max(np.sqrt(np.sum(point_set ** 2, axis = 1)),0)              #for each point conpute it's distance with respect to the origin anc take the maximum\r\n",
        "        point_set = point_set / dist                                            #scale to obtain maximum distance 1\r\n",
        "\r\n",
        "        if self.data_augmentation:\r\n",
        "            theta = np.random.uniform(0,np.pi*2)\r\n",
        "            rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],[np.sin(theta), np.cos(theta)]])\r\n",
        "            point_set[:,[0,2]] = point_set[:,[0,2]].dot(rotation_matrix) # random rotation\r\n",
        "            point_set += np.random.normal(0, 0.02, size=point_set.shape) # random jitter\r\n",
        "\r\n",
        "        seg = seg[choice]                             #random sampling for segment part \r\n",
        "\r\n",
        "        point_set = torch.from_numpy(point_set)                   #create pythorch tensor for pointcloud\r\n",
        "        seg = torch.from_numpy(seg)                               #create pythorch tensor for segmentation part\r\n",
        "        cls = torch.from_numpy(np.array([cls]).astype(np.int64))  #create pythorch tensor for classes id\r\n",
        "\r\n",
        "        if self.classification:\r\n",
        "            return point_set, cls\r\n",
        "        else:\r\n",
        "            return point_set, seg\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.datapath)     #return the number of pointcloud\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ2jcNt8KfQW"
      },
      "source": [
        "## DGCN Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPULYc5sK9E9"
      },
      "source": [
        "### Edge Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h08XVSoRJmQ6"
      },
      "source": [
        "def knn(x, k):\r\n",
        "    inner = -2*torch.matmul(x.transpose(2, 1), x)       #innser.size()-> [batchsize,npoints,npoints]\r\n",
        "    xx = torch.sum(x**2, dim=1, keepdim=True)          #distanza rispetto all'origine  #eleva ogni coordinata al quadrato e poi somma le 3 coordinate per ogni punto -> [batchsize,nfeature (3),npoints]->[batchsize,1,nponts]\r\n",
        "    pairwise_distance = -xx - inner - xx.transpose(2, 1) #[batchsize,npoints,npoints] distanza ogni punto con tutti gli altri\r\n",
        " \r\n",
        "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k) ->  \r\n",
        "    return idx\r\n",
        "\r\n",
        "\r\n",
        "def get_graph_feature(x, k=20, idx=None):\r\n",
        "    batch_size = x.size(0)\r\n",
        "    num_points = x.size(2)\r\n",
        "    x = x.view(batch_size, -1, num_points)\r\n",
        "    if idx is None:\r\n",
        "        idx = knn(x, k=k)   # (batch_size, num_points, k)\r\n",
        "    device = torch.device('cuda')\r\n",
        "\r\n",
        "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points #[batchsize,1,1] containing [[[0]],[[1*num_points]],..[[batchsize-1*numpoints]]]\r\n",
        "\r\n",
        "    idx = idx + idx_base        #somma ad ogni matrice di distanze (una per ogni batchsize) un offset corripondente al numero punti*inice batch\r\n",
        "\r\n",
        "    idx = idx.view(-1)      #concatena tutto in un unico array\r\n",
        " \r\n",
        "    _, num_dims, _ = x.size()\r\n",
        "\r\n",
        "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\r\n",
        "    feature = x.view(batch_size*num_points, -1)[idx, :]   #associa ad ogni punto i k punti più vicini calcolati con knn i cui indici sono in idx, l'offset calcolato in preceenza serve per far si che punti di batch diversi non si influenzino a vicenda\r\n",
        "    feature = feature.view(batch_size, num_points, k, num_dims) \r\n",
        "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)  #\r\n",
        "    \r\n",
        "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous() #funzione h (theta*(xi-xj )+(fi)xi)\r\n",
        "  \r\n",
        "    return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hSmeG-RJzAv"
      },
      "source": [
        "### DGCN Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLQz-UOpJ-yj"
      },
      "source": [
        "class DGCNN(nn.Module):\r\n",
        "    def __init__(self, emb_dims=1024,k=20,dropout=0.5, output_channels=1024):\r\n",
        "        super(DGCNN, self).__init__()\r\n",
        "        self.k = k\r\n",
        "        \r\n",
        "        self.bn1 = nn.BatchNorm2d(64)\r\n",
        "        self.bn2 = nn.BatchNorm2d(64)\r\n",
        "        self.bn3 = nn.BatchNorm2d(128)\r\n",
        "        self.bn4 = nn.BatchNorm2d(256)\r\n",
        "        self.bn5 = nn.BatchNorm1d(emb_dims)\r\n",
        "\r\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\r\n",
        "                                   self.bn1,\r\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\r\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\r\n",
        "                                   self.bn2,\r\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\r\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\r\n",
        "                                   self.bn3,\r\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\r\n",
        "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\r\n",
        "                                   self.bn4,\r\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\r\n",
        "        self.conv5 = nn.Sequential(nn.Conv1d(512, emb_dims, kernel_size=1, bias=False),\r\n",
        "                                   self.bn5,\r\n",
        "                                   nn.LeakyReLU(negative_slope=0.2))\r\n",
        "        \r\n",
        "        #self.linear1 = nn.Linear(emb_dims*2, 512, bias=False)\r\n",
        "        #self.bn6 = nn.BatchNorm1d(512)\r\n",
        "        #self.dp1 = nn.Dropout(p=dropout)\r\n",
        "        #self.linear2 = nn.Linear(512, 256)\r\n",
        "        #self.bn7 = nn.BatchNorm1d(256)\r\n",
        "        #self.dp2 = nn.Dropout(p=dropout)\r\n",
        "        self.linear3 = nn.Linear(emb_dims*2, output_channels)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        #print(\"Start x\")\r\n",
        "        #print(x.size())\r\n",
        "        batch_size = x.size(0)\r\n",
        "        x = get_graph_feature(x, k=self.k)\r\n",
        "        #print(\"GRAPH1 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x = self.conv1(x)\r\n",
        "        #print(\"CONV1 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x1 = x.max(dim=-1, keepdim=False)[0]\r\n",
        "        #print(\"MAX x1\")\r\n",
        "        #print(x1.size())\r\n",
        "\r\n",
        "        x = get_graph_feature(x1, k=self.k)\r\n",
        "        #print(\"GRAPH2 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x = self.conv2(x)\r\n",
        "        #print(\"CONV1 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x2 = x.max(dim=-1, keepdim=False)[0]\r\n",
        "        #print(\"STEP 2 x2\")\r\n",
        "        #print(x2.size())\r\n",
        "\r\n",
        "        x = get_graph_feature(x2, k=self.k)\r\n",
        "        #print(\"GRAPH3 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x = self.conv3(x)\r\n",
        "        #print(\"CONV3 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x3 = x.max(dim=-1, keepdim=False)[0]\r\n",
        "        #print(\"MAX X3\")\r\n",
        "        #print(x3.size())\r\n",
        "\r\n",
        "\r\n",
        "        x = get_graph_feature(x3, k=self.k)\r\n",
        "        #print(\"GRAPH4 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x = self.conv4(x)\r\n",
        "        #print(\"CONV3 x\")\r\n",
        "        #print(x.size())\r\n",
        "        x4 = x.max(dim=-1, keepdim=False)[0]\r\n",
        "        #print(\"MAX X4\")\r\n",
        "        #print(x4.size())\r\n",
        "\r\n",
        "\r\n",
        "        x = torch.cat((x1, x2, x3, x4), dim=1)\r\n",
        "        #print(\"CAT\")\r\n",
        "        #print(x.size())\r\n",
        "\r\n",
        "        x = self.conv5(x)\r\n",
        "        #print(\"MLP\")\r\n",
        "        #print(x.size())\r\n",
        "\r\n",
        "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\r\n",
        "       # x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\r\n",
        "       # x = torch.cat((x1, x2), 1)\r\n",
        "        #print(\"MAX POOLING X X1 X2\")\r\n",
        "        #print(x.size())\r\n",
        "        #print(x1.size())\r\n",
        "        #print(x2.size())\r\n",
        "\r\n",
        "        #x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\r\n",
        "        #x = self.dp1(x)\r\n",
        "        #x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\r\n",
        "        #x = self.dp2(x)\r\n",
        "       # x = self.linear3(x)\r\n",
        "        #print(\"END\")\r\n",
        "        #print(x.size())\r\n",
        "        return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFheDJG0O5v4"
      },
      "source": [
        "#### DGCN AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Y5hI0KO5C3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  ''' Just a lightweight Fully Connected decoder:\n",
        "  '''\n",
        "\n",
        "  def __init__(self, num_points = 2048):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_points = num_points\n",
        "        self.fc1 = nn.Linear(100, 128)\n",
        "        self.fc2 = nn.Linear(128, 256)\n",
        "        self.fc3 = nn.Linear(256, 512)\n",
        "        self.fc4 = nn.Linear(512, 1024)\n",
        "        self.fc5 = nn.Linear(1024, self.num_points * 3)\n",
        "        self.th = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "      batchsize = x.size()[0]\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = F.relu(self.fc2(x))\n",
        "      x = F.relu(self.fc3(x))\n",
        "      x = F.relu(self.fc4(x))\n",
        "      x = self.th(self.fc5(x))\n",
        "      x = x.view(batchsize, 3, self.num_points)\n",
        "      return x\n",
        "\n",
        "\n",
        "class DGCNN_AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, num_points=1024, feature_transform=False):\n",
        "    super(DGCNN_AutoEncoder, self).__init__()\n",
        "    print(\"DGCN AE Init - num_points (# generated): %d\" % num_points)\n",
        "\n",
        "    # Encoder Definition\n",
        "    self.encoder = torch.nn.Sequential(\n",
        "        DGCNN(),\n",
        "        nn.Linear(1024, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256, 100))\n",
        "    \n",
        "    # Decoder Definition\n",
        "    self.decoder = Decoder(num_points=num_points)\n",
        "\n",
        "  def forward(self, x):\n",
        "    BS, N, dim = x.size()\n",
        "    assert dim == 3, \"Fail: expecting 3 (x-y-z) as last tensor dimension!\"\n",
        "\n",
        "    # Refactoring batch for 'PointNetfeat' processing\n",
        "    x = x.permute(0, 2, 1)  # [BS, N, 3] => [BS, 3, N]\n",
        "\n",
        "    # Encoding\n",
        "    code = self.encoder(x)  # [BS, 3, N] => [BS, 100]\n",
        "\n",
        "    # Decoding\n",
        "    decoded = self.decoder(code)  # [BS, 3, num_points]\n",
        "\n",
        "    # Reshaping decoded output before returning..\n",
        "    decoded = decoded.permute(0,2,1)  # [BS, 3, num_points] => [BS, num_points, 3]\n",
        "\n",
        "    return decoded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wueb5wZlYFbe"
      },
      "source": [
        "batch_size = 32\n",
        "input_points = 1024\n",
        "\n",
        "# Instantiate a fake batch of point clouds\n",
        "points = torch.rand(batch_size, input_points, 3)\n",
        "print(\"Input points: \", points.size())\n",
        "f1=open(\"in.txt\",\"w\")\n",
        "arrayin=points.detach().cpu().numpy()\n",
        "arrayin.shape[1]\n",
        "for i in range(0,arrayin.shape[1]):\n",
        "  f1.write(str(arrayin[0,i,:][0])+' ')\n",
        "  f1.write(str(arrayin[0,i,:][1])+' ')\n",
        "  f1.write(str(arrayin[0,i,:][2])+' ')\n",
        "  f1.write('\\n')\n",
        "f1.close()\n",
        "# Instantiate the AE\n",
        "DGCNN_AE = DGCNN_AutoEncoder(num_points=input_points)\n",
        "\n",
        "# Move everything (data + model) to GPU\n",
        "assert torch.cuda.device_count() > 0, \"Fail: No GPU device detected\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "points = points.to(device)\n",
        "DGCNN_AE = DGCNN_AE.to(device) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdBUxA8vY10j"
      },
      "source": [
        "# try AE forward\n",
        "print(points.size())\n",
        "decoded = DGCNN_AE(points)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Decoded output: \", decoded.size())\n",
        "f=open(\"out.txt\",\"w\")\n",
        "arraynp=decoded.detach().cpu().numpy()\n",
        "arraynp.shape[1]\n",
        "print(str(arraynp[0,852,:]))\n",
        "print(str(arraynp[0,853,:]))\n",
        "for i in range(0,arraynp.shape[1]):\n",
        "  f.write(str(arraynp[0,i,:][0])+' ')\n",
        "  f.write(str(arraynp[0,i,:][1])+' ')\n",
        "  f.write(str(arraynp[0,i,:][2])+' ')\n",
        "  f.write('\\n')\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsSW-N5abNlu"
      },
      "source": [
        "## Loss Function - Chamfer\n",
        "Given the Original Point Cloud (PC) X and the decoded output Y we need a way to penalize the predicted/decoded w.r.t. the original PC X.<br>\n",
        "Standard MSE (Mean Squared Error) can't work cause both input and output are unordered sets of xyz-points.<br>\n",
        "Think at PCs as vectors of xyz-points, the order of each of the points is absolutely meaningless.\n",
        "Thus meaning that, since the lack of ordering for both input and decoded sets, you cannot simply compute the euclidean distance between the i-th element of the input tensor X and the i-th one of the decoded tensor Y.<br>\n",
        "We'll use the Chamfer Distance as training loss to keep the generated/decoded points close to original/(not encoded) points.\n",
        "The Chamfer Distance is not actually a distance but a pseudo-distance (dist(A,B) != dist(B,A)).<br>\n",
        "Let's consider the two sets X and Y. For each point in X we'll compute the euclidean distance to the nearest in the Y set. The same in the opposite direction, given each point of Y we'll compute the distance to the nearest in the X set. By doing so we're not making any assumption on the ordering of the X and Y sets.<br>\n",
        "The final loss will be the sum of two equally weighted contribution:\n",
        "1.   AVG_Y2X - Average of all nearest-dist Y->X: for each point in Y consider the distance to the nearest in X\n",
        "2.   AVG_X2Y - Average of all nearest-dist X->Y: same as 1. but opposite\n",
        "\n",
        "Thus:\n",
        "\n",
        "> CD_Loss = cd_weight * ( 0.5 * AVG_Y2X + 0.5 * AVG_X2Y )\n",
        "\n",
        "Where 'cd_weight' is a loss weight term that you should cross-validate. Hint: try with cd_weight=100!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qx_MvI4bNEz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Simple Chamfer Distance / Loss implementation\n",
        "# Implementation from https://github.com/zztianzz/PF-Net-Point-Fractal-Network/blob/master/utils.py\n",
        "# CVPR2020 Paper PF-Net: Point Fractal Network for 3D Point Cloud Completion.\n",
        "\n",
        "def array2samples_distance(array1, array2):\n",
        "    \"\"\"\n",
        "    arguments: \n",
        "        array1: the array, size: (num_point, num_feature)\n",
        "        array2: the samples, size: (num_point, num_feature)\n",
        "    returns:\n",
        "        distances: each entry is the distance from a sample to array1 \n",
        "    \"\"\"\n",
        "    num_point1, num_features1 = array1.shape\n",
        "    num_point2, num_features2 = array2.shape\n",
        "    expanded_array1 = array1.repeat(num_point2, 1)\n",
        "    expanded_array2 = torch.reshape(\n",
        "            torch.unsqueeze(array2, 1).repeat(1, num_point1, 1),\n",
        "            (-1, num_features2))\n",
        "\n",
        "    #k=1024*1024\n",
        "    distances = (expanded_array1-expanded_array2)* (expanded_array1-expanded_array2)\n",
        "    #print(distances.shape)\n",
        "    #distances = torch.sqrt(distances)\n",
        "    distances = torch.sum(distances,dim=1)\n",
        "    distances = torch.reshape(distances, (num_point2, num_point1))\n",
        "    distances = torch.min(distances,dim=1)[0]\n",
        "    distances = torch.mean(distances)\n",
        "    return distances\n",
        "\n",
        "def chamfer_distance_numpy(array1, array2):\n",
        "    batch_size, num_point, num_features = array1.shape\n",
        "    dist = 0\n",
        "    cd_weight = 100\n",
        "    for i in range(batch_size):\n",
        "        av_dist1 = array2samples_distance(array1[i], array2[i])\n",
        "        av_dist2 = array2samples_distance(array2[i], array1[i])\n",
        "        dist = dist + cd_weight * (0.5*av_dist1+0.5*av_dist2)/batch_size\n",
        "    return dist\n",
        "\n",
        "class PointLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PointLoss,self).__init__()\n",
        "        \n",
        "    def forward(self,array1,array2):\n",
        "        return chamfer_distance_numpy(array1,array2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_2FiBm_a3dX"
      },
      "source": [
        "### Chamfer distance example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci77bjtGhC--"
      },
      "source": [
        "chamfer_loss = PointLoss()  # instantiate the loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRLr0rY1hb0w"
      },
      "source": [
        "print(\"Input shape: \", points.size())\n",
        "print(\"Decoded shape: \", decoded.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrgYCM8IjivO"
      },
      "source": [
        "# let's compute the chamfer distance between the two sets: 'points' and 'decoded'\n",
        "loss = chamfer_loss(decoded, points)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iLLzPH5otU5"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4nO27VeayXq"
      },
      "source": [
        "## PointCloud Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0HLkZQ5amyc"
      },
      "source": [
        "def show_pointcloud(points,width=450,height=450,show_fig=True):\r\n",
        "  df = pd.DataFrame({'x':points[:, 0], 'y':points[:, 1], 'z':points[:, 2]})\r\n",
        "  df.head()\r\n",
        "  fig = px.scatter_3d(df, x='x', y='y', z='z',color='z',\r\n",
        "                       range_x=[-1,1], range_y=[-1,1], range_z=[-1,1],\r\n",
        "                      width=width, height=height)\r\n",
        "  fig.update_layout(scene_aspectmode='cube')\r\n",
        "  fig.update_traces(marker=dict(size=5),selector=dict(mode='markers'))\r\n",
        "  if show_fig==True:\r\n",
        "    fig.show()\r\n",
        "  else:\r\n",
        "    return fig\r\n",
        "\r\n",
        "\r\n",
        "def train_animation(points):\r\n",
        "  df = pd.DataFrame({'x':points[:, 0], 'y':points[:, 1], 'z':points[:, 2],'epoch':points[:,3]})\r\n",
        "  df.head()\r\n",
        "  fig = px.scatter_3d(df, x='x', y='y', z='z',color='z',animation_frame=\"epoch\",\r\n",
        "                      range_x=[-1,1], range_y=[-1,1], range_z=[-1,1],\r\n",
        "                      width=450, height=450)\r\n",
        "  fig.update_layout(scene_aspectmode='cube')\r\n",
        "  fig.update_traces(marker=dict(size=5),selector=dict(mode='markers'))\r\n",
        "  fig.show()\r\n",
        "\r\n",
        "\r\n",
        "#prova = np.loadtxt(\"animation.txt\", dtype=float)\r\n",
        "#train_animation(prova)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KEja2H88Iq1"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3YvJRU18Mmc"
      },
      "source": [
        "blue = lambda x: '\\033[94m' + x + '\\033[0m'\r\n",
        "\r\n",
        "batchSize=12\r\n",
        "n_workers=4\r\n",
        "load_state=False    #set to True to load a saved state\r\n",
        "epoch_done=0\r\n",
        "nEpoch=40\r\n",
        "categories=['Table']\r\n",
        "\r\n",
        "rootFolder='/content/gdrive/MyDrive/shapenetcore_partanno_segmentation_benchmark_v0'\r\n",
        "dataset = ShapeNetDataset(\r\n",
        "        root=rootFolder,\r\n",
        "        class_choice=categories,\r\n",
        "        classification=True,\r\n",
        "        split='train',\r\n",
        "        npoints=1024,\r\n",
        "        data_augmentation=False)\r\n",
        "\r\n",
        "\r\n",
        "val_dataset = ShapeNetDataset(\r\n",
        "        root=rootFolder,\r\n",
        "         class_choice=categories,\r\n",
        "        classification=True,\r\n",
        "        split='val',\r\n",
        "        npoints=1024,\r\n",
        "        data_augmentation=False)\r\n",
        "\r\n",
        "dataloader = torch.utils.data.DataLoader(\r\n",
        "    dataset,\r\n",
        "    batch_size=batchSize,\r\n",
        "    shuffle=True, \r\n",
        "    num_workers=n_workers)\r\n",
        "\r\n",
        "val_dataloader = torch.utils.data.DataLoader(\r\n",
        "    val_dataset,\r\n",
        "    batch_size=batchSize,\r\n",
        "    shuffle=False, \r\n",
        "    num_workers=n_workers)\r\n",
        "\r\n",
        "print(len(dataset), len(val_dataset))\r\n",
        "num_classes = len(dataset.classes)\r\n",
        "print('classes', num_classes)\r\n",
        "\r\n",
        "\r\n",
        "classifier=DGCNN_AutoEncoder()\r\n",
        "  \r\n",
        "if load_state == True:\r\n",
        "  f2=open(\"epoch.txt\",\"r\")\r\n",
        "  epoch_done=int(f2.read())\r\n",
        "  print(\"loading saved state, starting from epoch:\"+str(epoch_done))\r\n",
        "  classifier.load_state_dict(torch.load(\"model.params\"))\r\n",
        "\r\n",
        "\r\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999))           #alternativa a stocastic gradient descent\r\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)               #Decays the learning rate of each parameter group by gamma every step_size\r\n",
        "classifier.cuda()                  #classifier su gpu\r\n",
        "\r\n",
        "num_batch = len(dataset) / batchSize\r\n",
        "\r\n",
        "\r\n",
        "animation_points=np.empty([nEpoch*1024,4])\r\n",
        "print(animation_points.shape)\r\n",
        "\r\n",
        "f1=open(\"animation.txt\",\"w\")\r\n",
        "\r\n",
        "for epoch in range(epoch_done,nEpoch):\r\n",
        "    for i, data in enumerate(dataloader, 0):\r\n",
        "        points, target = data                                           #punti e risultato softmax\r\n",
        "        target = target[:, 0]                                           #c'è una sola classe ma è contenuta in un arraycon un solo elemento\r\n",
        "        points, target = points.cuda(), target.cuda()   \r\n",
        "        optimizer.zero_grad()                                           #resetta gradiente\r\n",
        "        classifier = classifier.train()                                 #train \r\n",
        "        decoded = classifier(points)                                    #decoded points\r\n",
        "        loss=chamfer_distance_numpy(decoded,points)\r\n",
        "        loss.backward()                                                 #calcolo gradiente\r\n",
        "        optimizer.step()\r\n",
        "        print('[%d: %d/%d] train loss: %f ' % (epoch, i, num_batch, loss.item()))\r\n",
        "\r\n",
        "        if i % 10 == 0:                                                 #ogni 10 batch\r\n",
        "            j, data = next(enumerate(val_dataloader, 0))                #prndo un singolo dato di testing\r\n",
        "            points, target = data\r\n",
        "            target = target[:, 0]\r\n",
        "            points, target = points.cuda(), target.cuda()\r\n",
        "            classifier = classifier.eval()                              #imposta il modello in modalità di validazione/testing (disattiva i batchnorm)\r\n",
        "            pred= classifier(points)                                   #recupera le predizioni\r\n",
        "            loss=chamfer_distance_numpy(decoded,points)                           #calcola loss\r\n",
        "            print('[%d: %d/%d] %s loss: %f ' % (epoch, i, num_batch, blue('step-validation'), loss.item()))\r\n",
        "\r\n",
        "    j, data = next(enumerate(val_dataloader, 0))                #prndo un batch di dati di testing\r\n",
        "    points, target = data\r\n",
        "    target = target[:, 0]\r\n",
        "    points, target = points.cuda(), target.cuda()\r\n",
        "    pred= classifier(points)                                   #recupera le predizioni\r\n",
        "    animation_points[epoch:epoch+1024]=np.concatenate((pred[0].cpu().detach().numpy().reshape(-1,3),np.full((1024,1),epoch)),axis=1)\r\n",
        "   \r\n",
        "    for i in range(epoch,epoch+1024):\r\n",
        "      f1.write(str(animation_points[i,0])+' ')\r\n",
        "      f1.write(str(animation_points[i,1])+' ')\r\n",
        "      f1.write(str(animation_points[i,2])+' ')\r\n",
        "      f1.write(str(animation_points[i,3])+'\\n')\r\n",
        "    \r\n",
        "    print(\"saving parameter\")\r\n",
        "    torch.save(classifier.state_dict(), \"model.params\")\r\n",
        "    print(\"parameter saved\")\r\n",
        "    f2=open(\"epoch.txt\",\"w\")\r\n",
        "    f2.write(str(epoch+1))\r\n",
        "    f2.close()\r\n",
        "    scheduler.step() \r\n",
        "    #print(epoch)\r\n",
        "f1.close() \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCb-fpMQuIo1"
      },
      "source": [
        "j, data = next(enumerate(val_dataloader, 0))   \r\n",
        "points=data[0][0].cpu().detach().numpy().reshape(-1,3)\r\n",
        "show_pointcloud(points)\r\n",
        "print(\"training history\")\r\n",
        "animation = np.loadtxt(\"animation.txt\", dtype=float)\r\n",
        "train_animation(animation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI8v_latvEWF"
      },
      "source": [
        "##Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42uBQWDvHFe"
      },
      "source": [
        "\r\n",
        "total_loss = 0\r\n",
        "for i,data in tqdm(enumerate(val_dataloader, 0)):       #tqdm è la progressbar (salta i dati di test già visti?)\r\n",
        "    points, target = data                               \r\n",
        "    target = target[:, 0]\r\n",
        "    points, target = points.cuda(), target.cuda()\r\n",
        "    classifier = classifier.eval()\r\n",
        "    decoded= classifier(points)\r\n",
        "    loss=chamfer_distance_numpy(decoded,points) \r\n",
        "    total_loss+=loss.item()\r\n",
        "\r\n",
        "total_loss=total_loss/len(val_dataloader)\r\n",
        "\r\n",
        "print(\"\\navg Validation loss {}\".format(total_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW3puOUPEgkz"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju-LhUltElgL"
      },
      "source": [
        "test_dataset={}\r\n",
        "test_dataloader={}\r\n",
        "category_loss={}\r\n",
        "\r\n",
        "for cat in categories:\r\n",
        "  test_dataset[cat] = ShapeNetDataset(\r\n",
        "          root=rootFolder,\r\n",
        "          class_choice=[cat],\r\n",
        "          classification=True,\r\n",
        "          split='test',\r\n",
        "          npoints=1024,\r\n",
        "          data_augmentation=False)\r\n",
        "  \r\n",
        "  test_dataloader[cat] = torch.utils.data.DataLoader(\r\n",
        "    test_dataset[cat],\r\n",
        "    batch_size=batchSize,\r\n",
        "    shuffle=False, \r\n",
        "    num_workers=n_workers)\r\n",
        "  \r\n",
        "  category_loss[cat]=0.0\r\n",
        "\r\n",
        "for cat in categories:\r\n",
        "  print(\"\\ntesting on \"+cat)\r\n",
        "  for i,data in tqdm(enumerate(test_dataloader[cat], 0)):       #tqdm è la progressbar (salta i dati di test già visti?)\r\n",
        "      points, target = data                               \r\n",
        "      target = target[:, 0]\r\n",
        "      points, target = points.cuda(), target.cuda()\r\n",
        "      classifier = classifier.eval()\r\n",
        "      decoded= classifier(points)\r\n",
        "      loss=chamfer_distance_numpy(decoded,points) \r\n",
        "      category_loss[cat]+=loss.item()\r\n",
        "  category_loss[cat]=category_loss[cat]/len(test_dataloader[cat])\r\n",
        "  print(\"\\nloss: \"+str(category_loss[cat]))\r\n",
        "\r\n",
        "  \r\n",
        "print(category_loss)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaymO1WrK0LB"
      },
      "source": [
        "show_example=5 #max 32\r\n",
        "\r\n",
        "i=0\r\n",
        "for cat in categories:\r\n",
        "  j, data = next(enumerate(test_dataloader[cat], 0))   \r\n",
        "  points,_ = data\r\n",
        "  points = points.cuda()\r\n",
        "  classifier = classifier.eval()\r\n",
        "  decoded= classifier(points) \r\n",
        "\r\n",
        "  for j in range (1,show_example+1):\r\n",
        "    loss=chamfer_distance_numpy(points[j].reshape(1,1024,3),decoded[j].reshape(1,1024,3))\r\n",
        "    s_points=points[j].cpu().detach().numpy()\r\n",
        "    s_decoded=decoded[j].cpu().detach().numpy()\r\n",
        "\r\n",
        "    grid = make_subplots(rows=1, cols=2,\r\n",
        "                     specs=[[{'type':'scene'},{'type':'scene'}]],\r\n",
        "                     subplot_titles=['Original','Decoded loss='+str(round(loss.item(),3 ))] )\r\n",
        "    grid.add_trace(\r\n",
        "      go.Scatter3d(x=s_points[:,0],\r\n",
        "                  y=s_points[:,1],\r\n",
        "                  z=s_points[:,2],\r\n",
        "                  mode=\"markers\", \r\n",
        "                  marker=dict(size=4,color=s_points[:,2],colorscale='Inferno'),\r\n",
        "                  ),row=1, col=1)\r\n",
        "    \r\n",
        "    grid.add_trace(\r\n",
        "      go.Scatter3d(x=s_decoded[:,0], \r\n",
        "                  y=s_decoded[:,1],\r\n",
        "                  z=s_decoded[:,2],\r\n",
        "                  mode=\"markers\",\r\n",
        "                  marker=dict(size=4,color=s_decoded[:,2],colorscale='Inferno'),\r\n",
        "                  \r\n",
        "                  ),row=1, col=2)\r\n",
        "    \r\n",
        "    grid.update_layout(height=400, width=800,showlegend=False,scene_aspectmode='cube')\r\n",
        "    grid.update_scenes(aspectmode='cube',\r\n",
        "                    xaxis = dict( range=[-1,1],),\r\n",
        "                     yaxis = dict( range=[-1,1],),\r\n",
        "                     zaxis = dict( range=[-1,1],),)\r\n",
        "    grid.show()\r\n",
        "\r\n",
        "  i=i+1\r\n",
        "\r\n",
        "\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}